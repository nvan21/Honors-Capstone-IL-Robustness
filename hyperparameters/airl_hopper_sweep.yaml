program: ./scripts/train_il.py # <-- CHANGE THIS to your main training script file name
method: bayes # Using Bayesian Optimization - usually best for expensive runs
metric:
  name: return/test # <-- CHANGE THIS if you log the final true reward under a different name
  goal: maximize # We want the highest possible true environment reward from the PPO policy

parameters:
  # --- PPO Specific Hyperparameters ---
  learning_rate:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-3
  gamma:
    distribution: uniform
    min: 0.95
    max: 0.999
  gae_lambda:
    distribution: uniform
    min: 0.9
    max: 0.99
  epoch_ppo:
    # Number of optimization epochs over the collected rollout data for PPO
    distribution: int_uniform
    min: 3
    max: 20
  coef_ent:
    # Entropy coefficient for PPO loss
    distribution: log_uniform_values
    min: 1e-4 # Can go lower if needed
    max: 0.05 # Usually not very high for PPO
  ppo_max_grad_norm:
    # Max gradient norm for PPO updates
    distribution: uniform
    min: 0.5
    max: 1.5 # Can increase if necessary, but 1.0 is common

  # --- Discriminator Specific Hyperparameters ---
  disc_lr:
    distribution: log_uniform_values
    min: 1e-5 # Discriminators often need smaller LRs than policy
    max: 5e-4
  epoch_disc:
    # Number of optimization epochs over the collected rollout data for the Discriminator
    # Adjust range based on how many updates you think are needed relative to PPO
    distribution: int_uniform
    min: 3
    max: 30 # Might need more epochs than PPO to keep up
  disc_max_grad_norm:
    # Max gradient norm for Discriminator updates
    distribution: uniform
    min: 0.5
    max: 1.5
  batch_size:
    # Mini-batch size for AIRL updates. Must be <= rollout_length
    # Using categorical for common power-of-2 values. Adjust as needed.
    distribution: categorical
    values: [64, 128, 256]

  # --- Shared/Loop Hyperparameters ---
  rollout_length:
    # Number of environment steps collected before each PPO/Disc update phase
    # Using categorical for common values. Ensure batch_size is compatible.
    distribution: categorical
    values: [512, 1024, 2048, 4096] # Adjust based on environment complexity & memory

# Optional: Early termination settings (can save compute)
early_terminate:
  type: hyperband
  min_iter: 3000000 # Require at least this many timesteps before considering termination
  # s: 2 # Number of brackets, adjust as needed

command:
  - ${env}
  - python
  - ${program}
  - --env=Hopper-v5
  - --experiment=normal_env
  - --algo=airl
