program: scripts/train_expert.py # <-- CHANGE to your SAC training script file name
method: bayes # Bayesian optimization is good for expensive RL runs
metric:
  name: return/test # Optimize for final TRUE environment reward achieved by SAC
  goal: maximize

parameters:
  # --- SAC Core Hyperparameters ---
  learning_rate: # Can use one LR for actor/critic or separate
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-3 # Common range, maybe try up to 5e-3 if needed
  # OR define separately:
  # actor_lr:
  #   distribution: log_uniform_values
  #   min: 1e-5
  #   max: 1e-3
  # critic_lr:
  #   distribution: log_uniform_values
  #   min: 1e-5
  #   max: 1e-3
  alpha_lr:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-3 # Common range, maybe try up to 5e-3 if needed

  gamma:
    # Discount factor - Usually kept high and often fixed, but can be tuned
    distribution: categorical # Or fixed value
    values: [0.99, 0.985, 0.98] # If fixed: value: 0.99

  buffer_size:
    # Replay buffer size
    distribution: categorical # Power-of-2 steps often make sense
    values: [100000, 500000, 1000000] # 1e6 is common

  batch_size:
    # Batch size for sampling from the replay buffer
    distribution: categorical
    values: [256, 512, 1024]

  tau:
    # Soft update coefficient for target networks
    distribution: uniform
    min: 0.001
    max: 0.01 # Usually small values, 0.005 is common default

  # --- Training Loop Hyperparameters ---
  start_steps:
    # Number of steps with random actions before training starts
    distribution: categorical
    values: [1000, 5000, 10000, 20000] # More initial exploration can help

  gradient_steps:
    # Number of gradient updates per environment step
    # Often kept at 1 for simplicity, but can be tuned
    distribution: categorical # Or fixed value: 1
    values: [1, 2, 4]

  # --- Optional: Network Architecture ---
  # You might want to sweep network sizes if defaults don't work
  # net_arch_actor:
  #   distribution: choice
  #   values: [[64, 64], [128, 128], [256, 256]]
  # net_arch_critic:
  #   distribution: choice
  #   values: [[64, 64], [128, 128], [256, 256]] # Often same as actor

  # --- Optional: Reward Scaling ---
  # You could make scaling a hyperparameter, though manual testing first is often easier
  # reward_scale_factor:
  #  distribution: choice
  #  values: [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]

  # --- Optional: Gradient Clipping ---
  # max_grad_norm:
  #   distribution: choice # Often fixed at 1.0 for SAC, but can be tuned
  #   values: [0.5, 1.0, 5.0]
# --- Optional: Early Termination (if runs are very long) ---
early_terminate:
  type: hyperband
  min_iter: 300000 # Min timesteps before considering stopping a trial

command:
  - ${env}
  - python
  - ${program}
  - --env=Hopper-v5
  - --experiment=modified_reward_hopper
